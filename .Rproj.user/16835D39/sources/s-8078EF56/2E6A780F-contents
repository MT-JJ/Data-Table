setwd("//Users/jason/Documents/aaaOLPD/Caring/SAMS Data Analysis/UCEA 2017")
library(foreign)
library(psych)
library(dplyr)
library(openxlsx)
library(ggplot2)
load("~/Documents/aaaOLPD/Data Sets/EFA_fns.RData")
'%notin%'<-Negate('%in%')

###Read in data#########
raw_data<-data.table(read.xlsx("SAMS_2016_Updated.xlsx",3,na.strings ="blank"))[-c(275,276,278,411,413),]

##Missing Data on all: c("275,276,278,411,413")

data_dict<-data.table(read.csv("SAMS_data_dictionary.csv",stringsAsFactors = F))
excluded_items<-data_dict[grep("SAMS|CE 6|Ours",data_dict$Source),"Variable"]
###Extract the items for factor analysis
items<-raw_data[,grep("^Q",colnames(raw_data),value=T),with=F]

Focused_items<-items[,-c(unlist(excluded_items),"Q2_18","Q3_10","Q3_11"),with=F]



###Check correlations#####
###Correlation function that includes p-values for listwise deletion only  Need to change degrees of freedom if pairwise which is problematic. If missing data run with pairwise after as sensitivity check.
corrs<-cor.prob(Focused_items,use="pairwise")
corrs_spear<-cor.prob(Focused_items,use="pairwise",method = "spearman")
###Check to see if any items have correlations below certain threshold and consider excluding such items from analysis. Assumes results based on corAll.
cor_cutoffs<-c(0.5,0.6)
Reduced_corr<-cor_check(corrs,cor_cutoffs)
Reduced_corr_spear<-cor_check(corrs_spear,cor_cutoffs)
#write.xlsx(Reduced_corr,"Test.xlsx",colNames=T,rowNames=F,colWidths="auto")


varlists<-lapply(Reduced_corr,colnames)
vardata<-setattr(lapply(1:length(varlists),function(x) data.table(raw_data)[,varlists[[x]][-1],with=F]),"names",names(varlists))
clean_data<-vardata[[1]]
Dof_analysis<-lapply(vardata,Ident_fn)

fa_cor<-matrix(Reduced_corr_spear$All_cors,ncol=length(varlists$All_cors))
#Measure of Sampling Adequacy- below 0.5 is unacceptable
MSA_KMO<-lapply(lapply(vardata[1],KMO),"[",c(1:2))
Bartlett_sphere_test<-data.table(do.call(rbind,lapply(vardata,function(i) data.frame(cortest.bartlett(i))[,c(3,1,2)])),keep.rownames = "Scenario")

##Based on pairwise results - do not use reduced correlation matrix (i.e., diagonals=SMC) for Kaiser.
Kaiser_C<-setattr(lapply(1:length(vardata),function(x) eigen(cor(vardata[[x]],use="pairwise.complete.obs"))$values),"names",names(varlists))

#May want to use reduced correlation matrix here (i.e., replace diagnals with the smc's according to Fabrigar et al, 1999)
Scree_Plots<-setattr(lapply(1:length(vardata),function(x) scree(vardata[[x]],pc=F,main=paste("Scree Plot for Correlations Above",cor_cutoffs[x]))),"names",names(varlists))

####If you get an error it is because one of the scenarios is suspect. Note the rationale is that a factor should account for more variance than is expected by chance.  Random eigenvalues have identical sample size and number of items.
Parallel_Analysis<-fa.parallel(vardata[[1]],fm="ml",fa="fa",SMC=T)
####VSS analysis to determine number of factors - may get errors for reduced cases so will need to shrink n. If you want only one scenario (i.e., all corr) then enter the vardata element as list. For given complexity creates matrix with highest loading and 0s for rest.
VSS_analysis_var<-VSS_fn(vardata[1],n=30,rotate = "varimax",diagonal = F,fm="ml",n.obs=NULL,plot=TRUE,use="pairwise",cor="cor") #Suggests 18

VSS_analysis_obl<-VSS_fn(vardata[1],n=30,rotate = "oblimin",diagonal = F,fm="ml",n.obs=NULL,plot=TRUE,use="pairwise",cor="cor") 

VSS_analysis_pro<-VSS_fn(vardata[1],n=30,rotate = "promax",diagonal = F,fm="ml",n.obs=NULL,plot=TRUE,use="pairwise",cor="cor") 

write.xlsx(unlist(list(VSS_analysis_var,VSS_analysis_obl,VSS_analysis_pro),recursive = F),"VSS.xlsx",colNames=T,rowNames=T,colWidths="auto")

###Now you can run the EFA on the appropriate subset(s) and choose a vector of factors to check scenarios.  Should also start with ML because of fit indices but be careful if you get improper solutions and data is not multivariate normal.  Since improper solutions may mark more serious issues, ML is a good starting point.
nfactor_scenarios<-c(12)

efaAll_pro<-setattr(lapply(1:length(nfactor_scenarios),function (x) fa(Remove_stars_corr(Reduced_corr$All_cors),n.obs=408,nfactors=nfactor_scenarios[x],rotate="promax",fm="ml")),"names",paste(nfactor_scenarios,"Factor Scenario",sep="-"))

efaAll_pro_spear<-setattr(lapply(1:length(nfactor_scenarios),function (x) fa(Remove_stars_corr(Reduced_corr$corrs_spear),n.obs=408,nfactors=nfactor_scenarios[x],rotate="promax",fm="ml")),"names",paste(nfactor_scenarios,"Factor Scenario",sep="-"))


efaAll_oblimin<-setattr(lapply(1:length(nfactor_scenarios),function (x) fa(vardata$All_cors,nfactors=nfactor_scenarios[x],rotate="oblimin",fm="ml")),"names",paste(nfactor_scenarios,"Factor Scenario",sep="-"))

###Here is a subset of the info contained in the list of efa runs
efaAll_sub_pro<-lapply(efaAll_pro,efa_sum_fn)
efaAll_sub_oblimin<-lapply(efaAll_oblimin,efa_sum_fn)

write.xlsx(unlist(efaAll_sub_pro,recursive = F),"EFAsummaryPro.xlsx",colNames=T,rowNames=T,colWidths="auto")

write.xlsx(unlist(efaAll_sub_oblimin,recursive = F),"EFAsummaryOblimin.xlsx",colNames=T,rowNames=T,colWidths="auto")

efaAll_rawload_ob<-lapply(lapply(lapply(efaAll_oblimin,loadings),function (i) matrix(i,ncol=ncol(i),dimnames=list(row.names(i),colnames(i)))),data.frame)

#write.xlsx(efaAll_rawload_ob,"Test.xlsx",colNames=T,rowNames=T,colWidths="auto")

efaAll_com_ob<-lapply(lapply(lapply(efaAll_oblimin,"[[","communality"),function(i) data.table(setnames(data.frame(i),"Communality"),keep.rownames = "Items")),function(j) rbind(j,list("Mean",mean(j$Communality))))


efa_com_summary_ob<-setnames(Reduce(function(dtf1,dtf2) data.table(merge(dtf1,dtf2,by="Items")),efaAll_com_ob),c("Items",names(efaAll_com_ob)))

write.xlsx(efa_com_summary_ob,"EFA_com_ob.xlsx",colNames=T,rowNames=T,colWidths="auto") #Mean communality is a way to estimate the total % of variance accounted for by all the factors for oblique rotations


#Linking item descriptions to items:
var_index<-setnames(data.table(read.xlsx("SAMS_2016_Updated.xlsx",1,na.strings ="blank")[,-1]),c("Items","Item_Desc"))

efaAll_load_ob<-lapply(lapply(mapply(function(X,Y) merge(X,Y,by=c("Items"),all.y=T),X=lapply(efaAll_rawload_ob,function(i) print_load(i,0.451)),Y=efaAll_com_ob),function(l) var_index[l,on="Items"]),function(k) setorder(k,ID)[,-"ID"])#mapply great for merging lists of dataframes



####For Final printing bc mapply breaks down if only one list element
#####Taking specific final scenario:
efaAll_rawload_final<-efaAll_rawload_ob[[1]]
efaAll_com_final<-efaAll_com_ob[[1]]
#Here is where you could relax the 0.451 too
efa_load_ob_final<-setorder(merge(print_load(efaAll_rawload_final,0.451),efaAll_com_final,by="Items",all.y=T),ID)[,-"ID",with=F]

item_reduction_fn<-function(X){
  if(is.data.table(X)==F) {stop("X is not a data.table")}
  dt<-copy(X)[,test_col:=rowSums(sapply(efa_load_ob_final,function(i) i==0))][test_col!=length(grep("Factor",colnames(X))),-c("test_col"),with=F]
  ex_items<-unname(unlist(copy(X)[,test_col:=rowSums(sapply(efa_load_ob_final,function(i) i==0))][test_col==length(grep("Factor",colnames(X))),c("Items"),with=F]))
  return(list(Reduced_set=dt,Removed_items=ex_items))
}
###This stores the removed items and clean data items for reanalysis
reducedefa1<-item_reduction_fn(efa_load_ob_final)

###Rerun without those items anf check to see if missing items.
missing_check1<-copy(clean_data)[,unlist(reducedefa1$Reduced_set$Items),with=F][,n_missings:=rowSums(sapply(copy(clean_data)[,unlist(reducedefa1$Reduced_set$Items),with=F],function(i) is.na(i)))]
max(missing_check1$n_missings)#none
efa_reduced1<-fa(missing_check1[,-c("n_missings"),with=F],nfactors=12,rotate="oblimin",fm="ml")

efa_reduced1_rload<-data.frame(matrix(loadings(efa_reduced1),ncol=ncol(loadings(efa_reduced1)),dimnames=list(row.names(loadings(efa_reduced1)),colnames(loadings(efa_reduced1)))))

efa_reduced1_load<-setnames(copy(data_dict),"Variable","Items")[print_load(efa_reduced1_rload,0.451),on="Items"]

#####Gathering Factor Means and naming factors############
#Creating lists for each factor item
factor_names<-c("Teacher Support","Aspiration","Peer Support","Academic Press","Family Support","Safety","Effort","Boredom","Anxiety","Distraction","Frustration")##Need to omit 12
Factorlists<-setattr(lapply(efa_reduced1_load,function(i) efa_reduced1_load[which(i!=0),])[grep("Factor",colnames(efa_reduced1_load))][-12],"names",factor_names)

#Extracting items from clean data for data analysis
Factorlists_data<-lapply(lapply(Factorlists,"[[","Items"),function(x) copy(raw_data)[,c(1,4:13,which(colnames(raw_data)%in%x)),with=F])

fscore_mean_fn<-function(X){
  fac_names<-names(X)
  Fac_summaries<-lapply(lapply(seq_along(X),function(i) X[[i]][,fac_names[i]:=apply(copy(X[[i]])[,grep("^Q",colnames(X[[i]]),value = T),with=F],1,mean)]),function(j) j[,c(1,length(j)),with=F])
  All_fscores<-dcast(na.omit(melt(rbindlist(Fac_summaries,fill=T),id.vars=c("X1"))),X1~variable)[,X1:=as.character(X1)]
  return(copy(raw_data)[,c(1,4:13),with=F][All_fscores,on="X1"])
}

Fscore_data<-fscore_mean_fn(Factorlists_data)[,Race:=ifelse(is.na(Race),"Unknown",Race)][,Race:=car::recode(Race,"'1'='Asian';'2'='Black';'3'='Hispanic';'4'='HI';'5'='White'")][,Gender:=ifelse(is.na(Gender),"Unknown",Gender)][,Gender:=car::recode(Gender,"'1'='Female';'2'='Male'")][,Grade:=car::recode(Grade,"1='6th';2='7th';3='8th'")]##3 students dropped out but need ot look

###Looking at why negative##
skinner<-setkey(copy(data_dict)[which(grepl("^[A-Z]{2}$",Source)),],Source)

Mind_states<-skinner[data.table(setNames(data.frame(t(sapply(copy(raw_data)[,unname(unlist(skinner$Variable)),with=F],function(i) cbind(Means=t(mean(i,na.rm=T)),SD=t(sd(i,na.rm=T)),Missing=t(sum(is.na(i))))))),c("Means","SD","Missing")),keep.rownames = "Variable"),on="Variable"]


#######Fscores by Race###########
Summ_function<-function(X,y){
  Mean<-copy(X)[,lapply(.SD,function(i) mean(i,na.rm=T)),by=y,.SDcols=12:22][,Stat:="Mean"]
  SD<-copy(X)[,lapply(.SD,function(i) sd(i,na.rm=T)),by=y,.SDcols=12:22][,Stat:="SD"]
  Missing<-copy(X)[,lapply(.SD,function(i) sum(is.na(i))),by=y,.SDcols=12:22][,Stat:="Missing"]
  N<-copy(X)[,lapply(.SD,function(i) .N-sum(is.na(i))),by=y,.SDcols=12:22][,Stat:="N"]
  Combined<-dcast(melt(rbindlist(list(Mean,SD,N,Missing)),id.vars=c(y,"Stat"))
  ,paste(paste(y,"Stat",sep = "+"),"~","variable"))
}
Race_summ<-Summ_function(Fscore_data,"Race")
Gender_summ<-Summ_function(Fscore_data,"Gender")
Grade_summ<-Summ_function(Fscore_data,"Grade")





##Removed the items here but need to make code better
final_excluded<-c(unlist(copy(final)[c(58:81),"Items",with=F]),unlist(excluded_items))


write.xlsx(final,"EFA_final.xlsx")
#write.xlsx(efaAll_load_ob,"Focused_EFA_load_ob0.451.xlsx",colNames=T,rowNames=F,colWidths="auto")

reduced_set<-copy(clean_data)[,which(colnames(clean_data)%notin%final_excluded),with=F]

efaAll_oblimin_final<-fa(reduced_set,nfactors=12,rotate="oblimin",fm="ml")

efaAll_rawload_final<-data.frame(matrix(loadings(efaAll_oblimin_final),ncol=ncol(loadings(efaAll_oblimin_final)),dimnames=list(row.names(loadings(efaAll_oblimin_final)),colnames(loadings(efaAll_oblimin_final)))))

###This is the set of variables we used with Karen - 11-8-2017

test<-setnames(copy(data_dict),"Variable","Items")[print_load(efaAll_rawload_final,0.451),on="Items"]


#####Based on Meeting with Ernest: Need to first check for higher order factors with variables we grouped and then keep in all the items we think measure these factors.  Add: Q10_6, 

efaAll_resid_ob<-setattr(lapply(lapply(1:length(efaAll_oblimin),function(i) data.table(unclass(residuals(efaAll_oblimin[[i]]))*lower.tri(unclass(residuals(efaAll_oblimin[[i]])),diag=T),keep.rownames = T)),function(j) mutate_if(j,is.numeric,function(k) na_if(k,0))),"names",names(efaAll_oblimin))

#write.xlsx(efaAll_resid,"EFA_resids.xlsx",colNames=T,rowNames=F,colWidths="auto")

#Note PCA may be good sensitivity analysis and if loadings are close (i.e., PCA and EFA are similar when common variance is high and each factor has many indicators) to EFA.  If so then PCA allows better score estimates.


####Factor Scores#######
#If using course methods, use the pattern matrix and not the structure matrix (Grice, 2001).  Indeterminancy exists because infinite number of sets of scores can be constructed for a given factor structure that are consistent with the loadings. If indeterminancy exceeds 50% then it is possible to construct two orthogonal or negatively correlated sets of scores consistent with the same loadings.  Small indeterminancy means each set of scores are highly correlated.

#Possible Refined Methods - (see Grice, 2001, p. 433)
#1) Thurstone (1935) Regression: Wkf = inv(Rkk)*Skf -item intercorrelations and loadings: the estimated factor scores will often be intercorrelated when the factors are orthogonal and they will not have unit variance. Just use the results from fa.

#2) Anderson & Rubin (1956) have factor scores constrained to orthogonality. This was extended to oblique by ten Berge, et all (1999) - see equation 8 of Grice: This equation is appropriate even when the covariance matrix for the unique factors is nonsingular, although in practice this matrix is assumed to be diagonal. The correlations among estimated factor scores computed from the weight matrix in Equation 8 are constrained to match the correlations among the factors themselves.
##Use for orthogonal rotations
Anderson_Rubin<-setattr(lapply(1:length(efaAll_oblimin),function (i) factor.scores(x=vardata[[1]],f=efaAll[[i]]$loadings,Phi = efaAll_oblimin[[i]]$Phi,method="Anderson")),"names",names(efaAll_oblimin))
for(i in 1:length(efaAll_oblimin)){
  Anderson_Rubin[[i]]$r<-efaAll_oblimin[[i]]$r
  Anderson_Rubin[[i]]$loadings<-efaAll_oblimin[[i]]$loadings
}
Anderson_indet<-lapply(Anderson_Rubin,function(i) Indeterminancy_fn(X=i,W=i$weights,Scores=i$scores))

#Use for oblique rotations
ten_berge<-setattr(lapply(1:length(efaAll_oblimin),function (i) factor.scores(x=vardata[[1]],f=efaAll_oblimin[[i]]$loadings,Phi = efaAll_oblimin[[i]]$Phi,method="tenBerge")),"names",names(efaAll_oblimin))
for(i in 1:length(efaAll_oblimin)){
  ten_berge[[i]]$r<-efaAll_oblimin[[i]]$r
  ten_berge[[i]]$loadings<-efaAll_oblimin[[i]]$loadings
}
tenBerge_indet<-lapply(ten_berge,function(i) Indeterminancy_fn(X=i,W=i$weights,Scores=i$scores))


#3) Bartlett (1937) developed a method that minimizes the sum of squares for the unique factors across the range of variables. Harman (1976) reports the "idealized variable" strategy based on the reproduced correlation matrix rather than on the original item correlations. These Correlations between the factors and noncorresponding factor score estimates computed from the weight matrices in Equations 9 and 10 are constrained to 0 when the factors are orthogonal. The correlations among the estimated factor scores, however, will not necessarily match the correlations among the factors, and the proportion of indeterminacy will not be minimized for either set of factor score estimates.

Bartlett<-setattr(lapply(1:length(efaAll),function (i) factor.scores(x=vardata[[1]],f=efaAll[[i]]$loadings,Phi = efaAll[[i]]$Phi,method="Bartlett")),"names",names(efaAll))
for(i in 1:length(efaAll)){
  Bartlett[[i]]$r<-efaAll[[i]]$r
  Bartlett[[i]]$loadings<-efaAll[[i]]$loadings
}
Bartlett_indet<-lapply(Bartlett,function(i) Indeterminancy_fn(X=i,W=i$weights,Scores=i$scores))


Harman<-setattr(lapply(1:length(efaAll),function (i) factor.scores(x=vardata[[1]],f=efaAll[[i]]$loadings,Phi = efaAll[[i]]$Phi,method="Harman")),"names",names(efaAll))
for(i in 1:length(efaAll)){
  Harman[[i]]$r<-efaAll[[i]]$r
  Harman[[i]]$loadings<-efaAll[[i]]$loadings
}
Harman_indet<-lapply(Harman,function(i) Indeterminancy_fn(X=i,W=i$weights,Scores=i$scores))

#4) Several other methods have also been developed (Heermann, 1963; Ledermann, 1939) that provide linear approximations of the factor scores using a complex, "refined" W^ weight matrix. Previous authors (e.g., Gorsuch, 1983; Grice & Harris, 1998; Horn, 1965) have referred to this general class of scores as exact factor scores. In the present article, however, these estimated factor scores will be referred to as "refined factor scores."

###Evaluating factor score approximations
#The multiple correlation between each factor and the original variables, p, as well as its square, p2 (Green, 1976; Mulaik, 1976), and (b) the minimum possible correlation between two sets of competing factor scores, 2p2 - 1 (Guttman, 1955; Mulaik, 1976; Schonemann, 1971).  Values for p that do not appreciably exceed 0.71 are therefore particularly problematic. These are the R2 info in fa.  But the default R2 are calculated as diag(t(weights)%*%loadings) and weights are regression weights which max correlations between factors and factor scores.  Would need to hand calculate if estimating weights using other methods.

Max_prop_det_ob<-lapply(efaAll_oblimin,"[[","R2")

Min_cor_fscores_ob<-lapply(Max_prop_det_ob,function(i) 2*i-1)

####the r.scores is close but not exactly what Grice (2001) suggests. Here is a function for determining validity and univocality.  Need to correlate the scores with the factors for correlational accuracy.

Indeterminancy_info_ob<-lapply(efaAll_oblimin,function(i) Indeterminancy_fn(i,pattern = F))

#Validity coefficients represent the correlations between the factor score estimates and their respective factors, and may range from -1 to +1. They should be interpreted in the same manner as p described previously. Gorsuch (1983, p.260) recommends values of at least .80, but much larger values (>.90) may be necessary if the factor score estimates are to serve as adequate substitutes for the factors themselves.

Validity_ob<-lapply(Indeterminancy_info_ob,"[[","Validity")

#Univocality: Another useful criterion for evaluating factor scores is univocality, which represents the extent to which the estimated factor scores are excessively or insufficiently correlated with other factors in the same analysis.  The idea from their example is that These results therefore indicate that the estimated factor scores are not heavily contaminated by variance from other orthogonal factors in the same analysis.

Univocality_ob<-lapply(Indeterminancy_info_ob,"[[","Univocality")

#Correlational accuracy: indicates the extent to which the correlations among the estimated factor scores match the correlations among the factors themselves. I think it is just the correlation of the .scores from results.

Corr_accuracy_ob<-lapply(Indeterminancy_info_ob,"[[","Cor_accuracy")

##For coarse estimates do not use the structure matrix but instead use the factor score coefficients (Wkf).




